{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_Price_Prediction.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyNtGw8/h9aiFEr+sxpRi4y9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed-derbeli/Machine-Learning-Projects/blob/main/Stock_Price_Prediction0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stock Price Prediction**\n",
        "\n",
        "\n",
        "*   In this project, we will be talking about predicting the returns on stocks. \n",
        "*   We will predict the stock price using the LSTM neural network.\n",
        "*   The data contains records about the stock price of Tata Global Beverages Limited. The dataset also contains a date-wise price of stock with open, close, high, and low prices along with volume traded as well as turnover on that day."
      ],
      "metadata": {
        "id": "N9fuB_sZ3XJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.    Imports"
      ],
      "metadata": {
        "id": "EDeiY1JEKCEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzEJB-8L3MoX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize']=20,10\n",
        "\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import LSTM,Dropout,Dense\n",
        "# from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Connection to mysql server"
      ],
      "metadata": {
        "id": "ybzFX8Vqc4fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: **Install Jupyter\n",
        "Install Jupyter on your local machine.\n",
        "\n",
        "**Step 2:** Install and enable the jupyter_http_over_ws jupyter extension (one-time)\n",
        "\n",
        "The jupyter_http_over_ws extension is authored by the Colaboratory team and available on GitHub.\n",
        "\n",
        "pip install jupyter_http_over_ws\n",
        "\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "\n",
        "**Step 3:** Start server and authenticate\n",
        "New notebook servers are started normally, though you will need to set a flag to explicitly trust WebSocket connections from the Colaboratory frontend.\n",
        "\n",
        "jupyter notebook \\\n",
        "\n",
        "  --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        "\n",
        "  --port=8888 \\\n",
        "\n",
        "  --NotebookApp.port_retries=0\n",
        "    \n",
        "Once the server has started, it will print a message with the initial backend URL used for authentication. Make a copy of this URL as you'll need to provide this in the next step.\n",
        "\n",
        "**Step 4:** Connect to the local runtime\n",
        "In Colaboratory, click the \"Connect\" button and select \"Connect to local runtime...\". Enter the URL from the previous step in the dialog that appears and click the \"Connect\" button. After this, you should now be connected to your local runtime."
      ],
      "metadata": {
        "id": "3BFrcQun6p6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables\n",
        "os.environ['MYSQL_DB_USERNAME'] = input(\"Enter the username of the environment variable:\")\n",
        "os.environ['MYSQL_DB_PASSWORD'] = input(\"Enter the password of the environment variable:\")\n",
        "\n",
        "# Get environment variables\n",
        "user_name = os.environ.get('MYSQL_DB_USERNAME')\n",
        "password = os.environ.get('MYSQL_DB_PASSWORD')"
      ],
      "metadata": {
        "id": "qzXvXl8JL7-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_"
      ],
      "metadata": {
        "id": "vNmLIxAp3ZCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE THE DATABASE CREDENTIALS\n",
        "user = user_name\n",
        "password = password\n",
        "host = 'localhost'\n",
        "port = 3306\n",
        "database = 'schema_2_'\n",
        "  \n",
        "# PYTHON FUNCTION TO CONNECT TO THE MYSQL DATABASE AND\n",
        "engine= create_engine(url=\"mysql+pymysql://{0}:{1}@{2}:{3}/{4}\".format(user, password, host, port, database))\n",
        "connection=engine.connect()\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "ul6zsVj_KtIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.   Read the dataset:"
      ],
      "metadata": {
        "id": "pXljIpit3cIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"SELECT * FROM schema_2_.Stock_Price_Predicton;\"\"\"\n",
        "\n",
        "df = pd.read_sql(query, con=connection)"
      ],
      "metadata": {
        "id": "ajLJCAut3bU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "KLZ9ABK45j1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 4. Analyze the closing prices from dataframe:"
      ],
      "metadata": {
        "id": "jKI8ZdtSB6aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Date\"]=pd.to_datetime(df.Date,format=\"%Y-%m-%d\")\n",
        "df.index=df['Date']\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df[\"Close\"],label='Close Price history')"
      ],
      "metadata": {
        "id": "_OBdv3NpAhDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Sort the date time & filter “Date” and “Close” columns:"
      ],
      "metadata": {
        "id": "7PY6OmGpC-UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=df.sort_index(ascending=True,axis=0)\n",
        "new_dataset=pd.DataFrame(index=range(0,len(df)),columns=['Date','Close'])\n",
        "\n",
        "for i in range(0,len(data)):\n",
        "    new_dataset[\"Date\"][i]=data['Date'][i]\n",
        "    new_dataset[\"Close\"][i]=data[\"Close\"][i]\n",
        "\n",
        "new_dataset.head"
      ],
      "metadata": {
        "id": "iknz6ckQDUKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Normalize the dataset:"
      ],
      "metadata": {
        "id": "pyL3CFS9E1xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaler=MinMaxScaler(feature_range=(0,1))\n",
        "# final_dataset=new_dataset.values\n",
        "# train_data=final_dataset[0:987,:]\n",
        "# valid_data=final_dataset[987:,:]\n",
        "# new_dataset.index=new_dataset.Date\n",
        "# new_dataset.drop(\"Date\",axis=1,inplace=True)\n",
        "# scaler=MinMaxScaler(feature_range=(0,1))\n",
        "# scaled_data=scaler.fit_transform(final_dataset)\n",
        "# x_train_data,y_train_data=[],[]\n",
        "# for i in range(60,len(train_data)):\n",
        "#     x_train_data.append(scaled_data[i-60:i,0])\n",
        "#     y_train_data.append(scaled_data[i,0])\n",
        "    \n",
        "# x_train_data,y_train_data=np.array(x_train_data),np.array(y_train_data)\n",
        "# x_train_data=np.reshape(x_train_data,(x_train_data.shape[0],x_train_data.shape[1],1))"
      ],
      "metadata": {
        "id": "9xjs7he0E9Py"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}